LINK TO YOUTUBE VIDEO: https://youtu.be/IvjkY0zC0rU

My entire project runs in a continuous webcam frame loop where MediaPipe Hands is used to extract 3D hand landmarks, and the normalized MediaPipe coordinates are then converted into pixel coordinates to perform calculations with. According to calculations made using the pixel coordinates, the program performs actions such as updating the drawing frame, clear frame and toggling the pen on or off. The canvas of drawn lines is maintained as a separate image from the webcam frame and then is merged onto the live frame for display.
The three most important components of my program include gesture recognition, canvas and drawing, and the screenshot folder. 
    Gesture recognition: By using MediaPipe to extract the coordinates of each hand-landmark, the Euclidean distance between specific fingertip landmarks is calculated (using NumPy’s built-in np.hypot function) by first converting the normalized coordinates into pixel coordinates. If the distance is less than 50 pixels -- a threshold determined experimentally -- the program identifies the thumb to be touching the fingertip and specific actions (such as clearing the frame, screenshotting, picking color) are performed.
    Canvas and drawing: The drawing uses cv2.line() to create line segments following the path that the index finger traces. The previous fingertip coordinates are stored in (prev_x, prev_y) so the line segment can connect the previous and current fingertip locations. The drawing canvas is maintained as a separate file from the camera frame in order to not lose any of the camera frame’s original color when combining the two frames. Specifically, when displaying drawing on top of the camera frame, the canvas is first converted to a grayscale image and then bitwise operations are used on the canvas frame to separate the drawing frame’s background and the strokes made. The strokes are then recombined with the camera frame, which was determined experimentally to produce a better visual effect than simply overlaying transparent layers.
    Screenshots: The folder icon is located on the upper right corner of the frame and created using screen ratios. Each frame follows the position of the mouse and checks if a click falls inside of the icon’s bounds. If so, the folder is opened using Python’s subprocess module. (I referred to ChatGPT for a lot of the technical implementation details and syntax for this part of the code).
Other notable technical details of the code include the decision to use MediaPipe, which played an important role in supporting the technical side of the project. MediaPipe was selected due to it being a reliable, pre-established model, being able to accurately detect multiple hands and fingers and having a built-in landmark model where the coordinates could be easily accessed. 
